With our project we try to analyze two environments of two different complexities and action space in order to catch up the parametrization of the training needed for an overall good result, by doing extensive parameter tunning and generating empirical simulations. The two environments are: Cart Pole and Bipedal Walker. The Cart Pole was analyzed using DQN and DQN-Experience Replay while the BipedalWalker was analyzed with the DDPG algorithm using an Actor-Critic agent and then using Augmented Random Search. For the first, We have found that parametrized implementations of these agents surpass their naive implementations. While for the Bipedal Walker, we found out that more tunnings are needed for the DDPG to achieve a certain amount of steps of the Bipedal, but with the Augmented Random Search achieve much better results are obtained in less time.

For our experiments, we found on the internet the implementation of each agent algorithm analyzed in this project. We also extended these implementations to make them more suitable to our way of working, computational resources and goals. Thus, our project does not aim to implement those algorithms from scratch, but to analyze their training parameters on the Cart Pole and BipedalWalker, for future experiments on equal or similar environments.
